---
layout: page
title: Projects
permalink: /project_page/
---



<div class="posts">

    <article class="post">

      <div class="entry">
        <h2>Useful Information:</h2>
      <p> This page will contain details regarding the Course Projects.
      </p>
      <p>Students are encouraged to select a topic and work on their own projects. However, it is highly recommended that students talk to members of various labs at IISc who are experienced in the topic they have selected.
      </p>
     <p> Projects can be broadly classified of two kinds:
     <ol>
	<li> <div style='font-weight: bold;'>Application</div>: when Deep learning is used for image processing in other research fields. For example, medical tool segmentation in operation videos.
        </li>
        <li> <div style='font-weight: bold;'>Improving Deep learning architectures</div>: when interesting new ideas are introduced to pre-existing models for improving their performance. For example, exploring benefits of higher dimensional convolution in a traditional classification setup.
      </li>
        </ol>
      </p>
      <p> <h3> As the course is on using Deep Learning for Computer Vision, the project must visual data in form of pixels etc.</h3></p>
      <h2>Resources </h2>
	<p> Some resources for selecting projects:<ul>
      <li><a href="https://github.com/kjw0612/awesome-deep-vision">Awesome Deep Vision</a></li>
      <li><a href="http://www.pamitc.org/cvpr14/accepted_papers.php">CVPR</a>: IEEE Conference on Computer Vision and Pattern Recognition</li>
      <li><a href="http://www.cvpapers.com/iccv2013.html">ICCV</a>: International Conference on Computer Vision</li>
      <li><a href="http://eccv2014.org/accepted-papers/">ECCV</a>: European Conference on Computer Vision</li>
      <li><a href="http://nips.cc/Conferences/2014/Program/accepted-papers.php">NIPS</a>: Neural Information Processing Systems</li>
      <li><a href="http://openreview.net/venue/iclr2014">ICLR</a>: International Conference on Learning Representations</li>
      <li><a href="http://www.kaggle.com/">Kaggle challenges</a>: An online machine learning competition website. For example, a <a href="https://www.kaggle.com/c/yelp-restaurant-photo-classification">Yelp classification challenge</a>.</li>
    </ul>
	</p>
<p>
For models, ConvNets have been successfully used in a variety of computer vision tasks. This type of projects would involve understanding the state-of-the-art vision models, and building new models or improving existing models for a vision task. The list below presents some papers on recent advances of ConvNets in the computer vision community.
</p>
<ul>
      <li><strong>Object recognition</strong>: <a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf">[Krizhevsky et al.]</a>, <a href="http://arxiv.org/abs/1409.0575">[Russakovsky et al.]</a>, <a href="http://arxiv.org/abs/1409.4842">[Szegedy et al.]</a>, <a href="http://arxiv.org/abs/1409.1556">[Simonyan et al.]</a>, <a href="http://arxiv.org/abs/1406.4729">[He et al.]</a></li>
      <li><strong>Object detection</strong>: <a href="http://arxiv.org/abs/1311.2524">[Girshick et al.]</a>, <a href="http://arxiv.org/abs/1312.6229">[Sermanet et al.]</a>, <a href="http://arxiv.org/abs/1312.2249">[Erhan et al.]</a></li>
      <li><strong>Image segmentation</strong>: <a href="http://arxiv.org/abs/1411.4038">[Long et al.]</a></li>
      <li><strong>Video classification</strong>: <a href="http://cs.stanford.edu/people/karpathy/deepvideo/">[Karpathy et al.]</a>, <a href="http://arxiv.org/abs/1406.2199">[Simonyan and Zisserman]</a></li>
      <li><strong>Scene classification</strong>: <a href="http://places.csail.mit.edu/">[Zhou et al.]</a></li>
      <li><strong>Face recognition</strong>: <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf">[Taigman et al.]</a></li>
      <li><strong>Depth estimation</strong>: <a href="http://www.cs.nyu.edu/~deigen/depth/">[Eigen et al.]</a></li>
      <li><strong>Image-to-sentence generation</strong>: <a href="http://cs.stanford.edu/people/karpathy/deepimagesent/">[Karpathy and Fei-Fei]</a>, <a href="http://arxiv.org/abs/1411.4389">[Donahue et al.]</a>, <a href="http://arxiv.org/abs/1411.4555">[Vinyals et al.]</a></li>
      <li><strong>Visualization and optimization</strong>: <a href="http://arxiv.org/pdf/1312.6199v4.pdf">[Szegedy et al.]</a>, <a href="http://arxiv.org/abs/1412.1897">[Nguyen et al.]</a>, <a href="http://arxiv.org/abs/1311.2901">[Zeiler and Fergus]</a>, <a href="http://arxiv.org/abs/1412.6572">[Goodfellow et al.]</a>, <a href="http://arxiv.org/abs/1312.6055">[Schaul et al.]</a></li>
    </ul></p>
<p>
We also provide a list of popular computer vision datasets:
<ul>
        <li><a href="http://www.cvpapers.com/datasets.html">Meta Pointer: A large collection organized by CV Datasets.</a></li>
        <li><a href="http://riemenschneider.hayko.at/vision/dataset/">Yet another Meta pointer</a></li>
        <li><a href="http://http://image-net.org/">ImageNet</a>: a large-scale image dataset for visual recognition organized by <a href="http://wordnet.princeton.edu/">WordNet</a> hierarchy</li>
        <li><a href="http://groups.csail.mit.edu/vision/SUN/">SUN Database</a>: a benchmark for scene recognition and object detection with annotated scene categories and segmented objects</li>
        <li><a href="http://places.csail.mit.edu/">Places Database</a>: a scene-centric database with 205 scene categories and 2.5 millions of labelled images</li>
        <li><a href="http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html">NYU Depth Dataset v2</a>: a RGB-D dataset of segmented indoor scenes</li>
        <li><a href="http://mscoco.org/">Microsoft COCO</a>: a new benchmark for image recognition, segmentation and captioning</li>
        <li><a href="http://yahoolabs.tumblr.com/post/89783581601/one-hundred-million-creative-commons-flickr-images">Flickr100M</a>: 100 million creative commons Flickr images</li>
        <li><a href="http://vis-www.cs.umass.edu/lfw/">Labeled Faces in the Wild</a>: a dataset of 13,000 labeled face photographs</li>
        <li><a href="http://human-pose.mpi-inf.mpg.de/">Human Pose Dataset</a>: a benchmark for articulated human pose estimation</li>
        <li><a href="http://www.cs.tau.ac.il/~wolf/ytfaces/">YouTube Faces DB</a>: a face video dataset for unconstrained face recognition in videos</li>
        <li><a href="http://crcv.ucf.edu/data/UCF101.php">UCF101</a>: an action recognition data set of realistic action videos with 101 action categories</li>
        <li><a href="http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/">HMDB-51</a>: a large human motion dataset of 51 action classes</li>
      </ul>
</p>
<h2>Grading Policy</h2>
<p> This information will be added soon. </p>
<h2> Some Example Projects</h2>
<p> Projects at VAL, by the present PhD. students will be added to this google doc. <a href="https://docs.google.com/document/d/1kmjYVqfCSuZbUC35R9PjxqcDh4cBdgOqGBmUbv6IrKE/edit?usp=sharing">Link to Google Doc</a></p>
<p> Projects in a similar course at Stanford has been listed <a href="http://cs231n.stanford.edu/2016/reports.html">here</a>. These can help guide you.
</p>
<h2>Acknowledgement</h2>
<p> Material from <a href="http://cs231n.stanford.edu/">CS231n</a> has been used for preparing this page, as well as the basic course material.</p>

